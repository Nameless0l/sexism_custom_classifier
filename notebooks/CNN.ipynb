{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([642, 46, 768])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 1000\n",
    "from src.data.make_dataset import MakeDataset\n",
    "from src.data.preprocessing.preprocess_bert import PreprocessBert\n",
    "from src.feature_extraction.build_bert_features import BuildBERTFeature\n",
    "from src.enums import * \n",
    "\n",
    "make_dataset = MakeDataset()\n",
    "data = make_dataset.read_data('../data/raw/all_data_augmented.csv')\n",
    "\n",
    "data=data[data['of_id'].isnull()]\n",
    "data=data[data['dataset'].isin(['benevolent', 'hostile', 'other'])]\n",
    "\n",
    "\n",
    "train_domain=Domain.BHO\n",
    "test_domain=Domain.BHO\n",
    "X_train_, y_train_, X_test_, y_test_=make_dataset.get_balanced_data_split(data, train_domain, test_domain)\n",
    "\n",
    "bf=BuildBERTFeature(output_hidden_states=False, extract=False, embedding_file_name='../src/bert_embeddings/word_embeddings20201121-161340.pkl')\n",
    "X_train=bf.transform(X_train_)\n",
    "X_test=bf.transform(X_test_)\n",
    "y_train=y_train_ \n",
    "y_test=y_test_ \n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_length 46\n",
      "embedding_size 768\n"
     ]
    }
   ],
   "source": [
    "sequence_length=X_train.shape[1]\n",
    "print('sequence_length {}'.format(sequence_length))\n",
    "\n",
    "embedding_size=X_train.shape[2]\n",
    "print('embedding_size {}'.format(embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 46, 768, 1)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 45, 767, 128)      640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 22, 383, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1078528)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                69025856  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 69,026,561\n",
      "Trainable params: 69,026,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#conv2d\n",
    "\n",
    "from tensorflow.keras import Input, layers, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "#vocabulary_size=1903\n",
    "kernel_sizes=[2, ]\n",
    "\n",
    "#model = tf.keras.Sequential()\n",
    "input_ = layers.Input(shape=(sequence_length, embedding_size, 1))\n",
    "\n",
    "pooled_outputs = []\n",
    "for kernel_size in kernel_sizes:\n",
    "    conv= layers.Conv2D(filters=128, kernel_size=int(kernel_size), activation='relu', padding='valid')(input_)    \n",
    "    pooled=layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    pooled_outputs.append(pooled)\n",
    "\n",
    "# Combine all the pooled features\n",
    "concs=None\n",
    "if len(kernel_sizes) > 1:\n",
    "    concs = layers.Concatenate(axis=1)(pooled_outputs)\n",
    "else:\n",
    "    concs=pooled_outputs[0]\n",
    "    \n",
    "flat =layers.Flatten()(concs)\n",
    "#drop =layers.Dropout(0.5)(flat)\n",
    "\n",
    "dense_=layers.Dense(64, activation='relu')(flat)\n",
    "#dense = layers.Dense(8, activation='relu')(drop)\n",
    "output_ = layers.Dense(1)(dense_)  \n",
    "#output_ = layers.Dense(1, activation='softmax')(drop)\n",
    "\n",
    "model_2d = Model(input_, output_)\n",
    "\n",
    "model_2d.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_2d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 189s 17s/step - loss: 14.0746 - accuracy: 0.5125\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 134s 12s/step - loss: 3.7971 - accuracy: 0.5607\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 147s 13s/step - loss: 1.4678 - accuracy: 0.6931\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 141s 13s/step - loss: 0.5201 - accuracy: 0.8271\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 140s 13s/step - loss: 0.2458 - accuracy: 0.8941\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 145s 13s/step - loss: 0.1736 - accuracy: 0.9268\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 150s 14s/step - loss: 0.1334 - accuracy: 0.9455\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 152s 14s/step - loss: 0.1072 - accuracy: 0.9720\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 149s 14s/step - loss: 0.1146 - accuracy: 0.9517\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 135s 12s/step - loss: 0.0761 - accuracy: 0.9875\n",
      "5/5 [==============================] - 17s 3s/step - loss: 0.5012 - accuracy: 0.7959\n",
      "Test Loss: 0.5011796951293945\n",
      "Test Accuracy: 0.795918345451355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67       147\n",
      "           1       0.00      0.00      0.00       147\n",
      "\n",
      "    accuracy                           0.50       294\n",
      "   macro avg       0.25      0.50      0.33       294\n",
      "weighted avg       0.25      0.50      0.33       294\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "#old shape : (642, 46, 768)\n",
    "#new shape (642, 46, 768, 1)\n",
    "X_train=X_train.numpy().reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_train=tf.convert_to_tensor(X_train)\n",
    "\n",
    "X_test=X_test.numpy().reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "X_test=tf.convert_to_tensor(X_test)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train , y_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test , y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "model_2d.fit(train_dataset, epochs=10)\n",
    "test_loss, test_acc = model_2d.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_pred=np.argmax(model_2d.predict(test_dataset), axis=-1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print( classification_report( y_test, y_pred,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
